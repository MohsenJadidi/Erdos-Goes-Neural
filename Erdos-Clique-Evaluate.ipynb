{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploring on hard instances full trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/karalias\n"
     ]
    }
   ],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from kernel.datasets import get_dataset\n",
    "import time\n",
    "from torch import tensor\n",
    "from torch.optim import Adam\n",
    "from torch.optim import SGD\n",
    "from torch_geometric.data import DataLoader, DenseDataLoader as DenseLoader\n",
    "from math import ceil\n",
    "from torch.nn import Linear\n",
    "from torch.distributions import categorical\n",
    "from torch.distributions import Bernoulli\n",
    "import torch.nn\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import pygraphviz as pgv\n",
    "from torch_geometric.utils import convert as cnv\n",
    "from torch_geometric.utils import sparse as sp\n",
    "from torch_geometric.data import Data\n",
    "import pygraphviz as pgv\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "import networkx as nx\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "from torch.nn.functional import gumbel_softmax\n",
    "from torch.distributions import relaxed_categorical\n",
    "from torch_geometric.nn.inits import uniform\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.nn import GINConv, GATConv\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU, LeakyReLU\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch.nn import Linear, Sequential, ReLU, BatchNorm1d as BN\n",
    "from torch_geometric.utils import scatter_\n",
    "from torch_geometric.data import Batch \n",
    "from torch_scatter import scatter_min, scatter_max, scatter_add, scatter_mean\n",
    "from torch import autograd\n",
    "from torch_geometric.utils import softmax, add_self_loops, remove_self_loops, segregate_self_loops, remove_isolated_nodes, contains_isolated_nodes, add_remaining_self_loops\n",
    "from models import cut_MPNN\n",
    "from modules_and_utils import derandomize_cut, GATAConv,get_diracs, derandomize_clique_final, derandomize_clique_final_speed, solve_gurobi_maxclique\n",
    "import scipy\n",
    "import scipy.io\n",
    "import GPUtil\n",
    "from networkx.algorithms.approximation import max_clique\n",
    "from torch_geometric.data import DataListLoader\n",
    "from random import shuffle\n",
    "from networkx.algorithms.approximation import max_clique\n",
    "from networkx.algorithms import graph_clique_number\n",
    "from networkx.algorithms import find_cliques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"TWITTER_SNAP\", \"COLLAB\", \"IMDB-BINARY\"]\n",
    "dataset_name = datasets[0]\n",
    "path_to_dataset =\"%PATH\"\n",
    "if dataset_name == \"TWITTER_SNAP\":\n",
    "    stored_dataset = open(path_to_dataset, 'rb')        \n",
    "elif dataset_name == \"COLLAB\":\n",
    "    stored_dataset = open(path_to_dataset, 'rb')\n",
    "elif dataset_name == \"IMDB-BINARY\":\n",
    "    stored_dataset = open(path_to_dataset, 'rb')\n",
    "\n",
    "dataset = pickle.load(stored_dataset)\n",
    "dataset_scale = 1\n",
    "total_samples = int(np.floor(len(dataset)*dataset_scale))\n",
    "dataset = dataset[:total_samples]\n",
    "\n",
    "num_trainpoints = int(np.floor(0.6*len(dataset)))\n",
    "num_valpoints = int(np.floor(num_trainpoints/3))\n",
    "num_testpoints = len(dataset) - (num_trainpoints + num_valpoints)\n",
    "traindata= dataset[0:num_trainpoints]\n",
    "valdata = dataset[num_trainpoints:num_trainpoints + num_valpoints]\n",
    "testdata = dataset[num_trainpoints + num_valpoints:]\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(traindata, batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testdata, batch_size, shuffle=False)\n",
    "val_loader =  DataLoader(valdata, batch_size, shuffle=False)\n",
    "\n",
    "#set up random seeds \n",
    "torch.manual_seed(1)\n",
    "np.random.seed(2)   \n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader, recfield):\n",
    "    model.train()\n",
    "    avg_loss = 0\n",
    "    avg_cliqdist = 0\n",
    "    exp_cardinalities = torch.tensor(0)\n",
    "    for data in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        #print(\"data  batchsum: \",(data.batch==1).sum())\n",
    "        data = get_diracs(data, 1, sparse = True, effective_volume_range=0.15, receptive_field = recfield)\n",
    "        data = data.to(device)\n",
    "        #print(\"data prime batchsum: \",(data.batch==1).sum())\n",
    "        retdict = model(data)\n",
    "        avg_loss += retdict['loss'][0].item()/len(data_loader)\n",
    "        avg_cliqdist += retdict[\"Currvol/Cliquevol\"][0].item()/len(data_loader)\n",
    "        exp_cardinalities = [retdict[\"Expected_cardinality_hist\"][0]]\n",
    "    return avg_loss, avg_cliqdist, exp_cardinalities \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD MODEL INSTEAD OF TRAINING\n",
    "net =  net =  cliqueMPNN_hindsight_earlyGAT(dataset,4, 64, 1 ,1)\n",
    "pretrained_model_name = \"%PATH\"\n",
    "\n",
    "cpoint = torch.load(pretrained_model_name)\n",
    "net.load_state_dict(cpoint[file_name], strict=False)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data = testdata\n",
    "test_loader = DataLoader(test_data, batch-size, shuffle=False)\n",
    "net.to(device)\n",
    "count = 1\n",
    "net.eval()\n",
    "\n",
    "gnn_nodes = []\n",
    "gnn_edges = []\n",
    "gnn_sets = {}\n",
    "#number of random samples\n",
    "max_samples = 1\n",
    "gnn_times = []\n",
    "num_samples = max_samples\n",
    "t_start = time.time()\n",
    "\n",
    "\n",
    "for data in test_loader:\n",
    "    num_graphs = data.batch.max().item()+1\n",
    "    bestset = {}\n",
    "    bestedges = np.zeros((num_graphs))\n",
    "    maxset = np.zeros((num_graphs))\n",
    "    \n",
    "    #generate samples\n",
    "    total_samples = []\n",
    "    for graph in range(num_graphs):\n",
    "        curr_inds = (data.batch==graph)\n",
    "        g_size = curr_inds.sum().item()\n",
    "        if max_samples <= g_size: \n",
    "            samples = np.random.choice(curr_inds.sum().item(),max_samples, replace=False)\n",
    "        else:\n",
    "            samples = np.random.choice(curr_inds.sum().item(),max_samples, replace=True)\n",
    "\n",
    "        total_samples +=[samples]\n",
    "\n",
    "    data = data.to(device)\n",
    "    t_0 = time.time()\n",
    "    for k in range(num_samples):\n",
    "        t_datanet_0 = time.time()\n",
    "        data_prime = get_diracs(data.to(device), 1, sparse = True, effective_volume_range=0.15, receptive_field = 7)\n",
    "        print(\"Number of nodes: \", data_prime.x.shape[0])\n",
    "        initial_values = data_prime.x.detach()\n",
    "        data_prime.x = torch.zeros_like(data_prime.x)\n",
    "        g_offset = 0\n",
    "        for graph in range(num_graphs):\n",
    "            curr_inds = (data_prime.batch==graph)\n",
    "            g_size = curr_inds.sum().item()\n",
    "            graph_x = data_prime.x[curr_inds]\n",
    "            data_prime.x[total_samples[graph][k] + g_offset]=1.\n",
    "            g_offset += g_size\n",
    "       \n",
    "        #forward_pass  \n",
    "        ret_dict = net(data_prime)\n",
    "        \n",
    "        #derandomize\n",
    "        t_datanet_1 = time.time() - t_datanet_0\n",
    "        print(\"data prep and fp: \", t_datanet_1)\n",
    "        t_derand_0 = time.time()\n",
    "        _, set_edges, set_cardinality = derandomize_clique_final_speed(data_prime,(ret_dict[\"output\"][0]), weight_factor =0.,draw=False, beam = 1)\n",
    "        t_derand_1 = time.time() - t_derand_0\n",
    "        print(\"Derandomization time: \", t_derand_1)\n",
    "        for j in range(num_graphs):\n",
    "            indices = (data.batch == j)\n",
    "            if (set_cardinality[j]>maxset[j]):\n",
    "                    maxset[j] = set_cardinality[j].item()\n",
    "                    bestset[str(j)] = sets[indices].cpu()\n",
    "                    bestedges[j] = set_edges[j].item()\n",
    "\n",
    "    t_1 = time.time()-t_0\n",
    "    print(\"Current batch: \", count)\n",
    "    print(\"Time so far: \", time.time()-t_0)\n",
    "    gnn_sets[str(count)] = bestset\n",
    "    \n",
    "    gnn_nodes += [maxset]\n",
    "    gnn_edges += [bestedges]\n",
    "    gnn_times += [t_1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    count += 1\n",
    "t_1 = time.time()\n",
    "total_time = t_1 - t_start\n",
    "print(\"Average time per graph: \", total_time/(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get clique numbers\n",
    "test_data_clique = []\n",
    "for data in testdata:\n",
    "    my_graph = to_networkx(Data(x=data.x, edge_index = data.edge_index)).to_undirected()\n",
    "    print(my_graph)\n",
    "    cliqno, _ = solve_gurobi_maxclique(my_graph, 100000)\n",
    "    data.clique_number = cliqno\n",
    "    test_data_clique += [data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten the output list\n",
    "flat_list = [item for sublist in gnn_edges for item in sublist]\n",
    "for k in range(len(flat_list)):\n",
    "    flat_list[k] = flat_list[k].item()\n",
    "gnn_edges = (flat_list)\n",
    "\n",
    "flat_list = [item for sublist in gnn_nodes for item in sublist]\n",
    "for k in range(len(flat_list)):\n",
    "    flat_list[k] = flat_list[k].item()\n",
    "gnn_nodes = (flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = test_data_clique\n",
    "ratios = [gnn_nodes[i]/tests[i].clique_number for i in range(len(tests))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean ratio: {(np.array(ratios)).mean()} +/-  {(np.array(ratios)).std()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Stalence)",
   "language": "python",
   "name": "stalence"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
