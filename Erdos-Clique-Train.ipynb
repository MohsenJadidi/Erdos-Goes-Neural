{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First take care of all imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from kernel.datasets import get_dataset\n",
    "import time\n",
    "from torch import tensor\n",
    "from torch.optim import Adam\n",
    "from torch.optim import SGD\n",
    "from torch_geometric.data import DataLoader, DenseDataLoader as DenseLoader\n",
    "from math import ceil\n",
    "from torch.nn import Linear\n",
    "from torch.distributions import categorical\n",
    "from torch.distributions import Bernoulli\n",
    "import torch.nn\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import pygraphviz as pgv\n",
    "from torch_geometric.utils import convert as cnv\n",
    "from torch_geometric.utils import sparse as sp\n",
    "from torch_geometric.data import Data\n",
    "import pygraphviz as pgv\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "import networkx as nx\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "from torch.nn.functional import gumbel_softmax\n",
    "from torch.distributions import relaxed_categorical\n",
    "from torch_geometric.nn.inits import uniform\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.nn import GINConv, GATConv\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU, LeakyReLU\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch.nn import Linear, Sequential, ReLU, BatchNorm1d as BN\n",
    "from torch_geometric.utils import scatter_\n",
    "from torch_geometric.data import Batch \n",
    "from torch_scatter import scatter_min, scatter_max, scatter_add, scatter_mean\n",
    "from torch import autograd\n",
    "from torch_geometric.utils import softmax, add_self_loops, remove_self_loops, segregate_self_loops, remove_isolated_nodes, contains_isolated_nodes, add_remaining_self_loops\n",
    "from models import cut_MPNN\n",
    "from modules_and_utils import derandomize_cut, GATAConv,get_diracs, derandomize_clique_final, derandomize_clique_final_speed, solve_gurobi_maxclique\n",
    "import scipy\n",
    "import scipy.io\n",
    "import GPUtil\n",
    "from networkx.algorithms.approximation import max_clique\n",
    "from torch_geometric.data import DataListLoader\n",
    "from random import shuffle\n",
    "from networkx.algorithms.approximation import max_clique\n",
    "from networkx.algorithms import graph_clique_number\n",
    "from networkx.algorithms import find_cliques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"TWITTER_SNAP\", \"COLLAB\", \"IMDB-BINARY\"]\n",
    "dataset_name = datasets[0]\n",
    "path_to_dataset =\"%PATH\"\n",
    "if dataset_name == \"TWITTER_SNAP\":\n",
    "    stored_dataset = open(path_to_dataset, 'rb')        \n",
    "elif dataset_name == \"COLLAB\":\n",
    "    stored_dataset = open(path_to_dataset, 'rb')\n",
    "elif dataset_name == \"IMDB-BINARY\":\n",
    "    stored_dataset = open(path_to_dataset, 'rb')\n",
    "\n",
    "dataset = pickle.load(stored_dataset)\n",
    "dataset_scale = 1\n",
    "total_samples = int(np.floor(len(dataset)*dataset_scale))\n",
    "dataset = dataset[:total_samples]\n",
    "\n",
    "num_trainpoints = int(np.floor(0.6*len(dataset)))\n",
    "num_valpoints = int(np.floor(num_trainpoints/3))\n",
    "num_testpoints = len(dataset) - (num_trainpoints + num_valpoints)\n",
    "traindata= dataset[0:num_trainpoints]\n",
    "valdata = dataset[num_trainpoints:num_trainpoints + num_valpoints]\n",
    "testdata = dataset[num_trainpoints + num_valpoints:]\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(traindata, batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testdata, batch_size, shuffle=False)\n",
    "val_loader =  DataLoader(valdata, batch_size, shuffle=False)\n",
    "\n",
    "#set up random seeds \n",
    "torch.manual_seed(1)\n",
    "np.random.seed(2)   \n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader, recfield):\n",
    "    model.train()\n",
    "    avg_loss = 0\n",
    "    avg_cliqdist = 0\n",
    "    exp_cardinalities = torch.tensor(0)\n",
    "    for data in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        data = get_diracs(data, 1, sparse = True, effective_volume_range=0.15, receptive_field = recfield)\n",
    "        data = data.to(device)\n",
    "        retdict = model(data)\n",
    "        avg_loss += retdict['loss'][0].item()/len(data_loader)\n",
    "        avg_cliqdist += retdict[\"Currvol/Cliquevol\"][0].item()/len(data_loader)\n",
    "        exp_cardinalities = [retdict[\"Expected_cardinality_hist\"][0]]\n",
    "    return avg_loss, avg_cliqdist, exp_cardinalities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_sizes = [32]\n",
    "l_rates = [0.001]\n",
    "depths = [4]\n",
    "coefficients = [4.]\n",
    "rand_seeds = [66]\n",
    "widths = [64]\n",
    "epochs = 100\n",
    "net.train()\n",
    "retdict = {}\n",
    "edge_drop_p = 0.0\n",
    "edge_dropout_decay = 0.90\n",
    "penalty_coeff = 9.00\n",
    "penalty_increase = -0.00\n",
    "validation_timeout = 75\n",
    "\n",
    "\n",
    "save_path = \"%PATH\"\n",
    "\n",
    "for batch_size, learning_rate, numlayers, penalty_coeff, r_seed, hidden_1 in product(b_sizes, l_rates, depths, coefficients, rand_seeds, widths):\n",
    "   \n",
    "    torch.manual_seed(r_seed)\n",
    "    train_loader = DataLoader(traindata, batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(testdata, batch_size, shuffle=False)\n",
    "    val_loader =  DataLoader(valdata, batch_size, shuffle=False)\n",
    "\n",
    "    receptive_field= numlayers + 1\n",
    "    val_losses = []\n",
    "    cliq_dists = []\n",
    "\n",
    "    hidden_2 = 1\n",
    "\n",
    "    net =  clique_MPNN(dataset,numlayers, hidden_1, hidden_2 ,1, elasticity = None)\n",
    "    net.to(device).reset_parameters()\n",
    "    optimizer = Adam(net.parameters(), lr=learning_rate, weight_decay=0.00000)\n",
    "    \n",
    "    plotter = VisdomLinePlotter(env_name=envname)\n",
    "    for epoch in range(epochs):\n",
    "        totalretdict = {}\n",
    "        count=0\n",
    "        if epoch % 5 == 0:\n",
    "            edge_drop_p = edge_drop_p*edge_dropout_decay\n",
    "            print(\"Edge_dropout: \", edge_drop_p)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            penalty_coeff = penalty_coeff + 0.\n",
    "            print(\"Penalty_coefficient: \", penalty_coeff)\n",
    "\n",
    "        if epoch % 15 == 0:\n",
    "            val_l, cliq_dist, val_cardinalities= predict(net, val_loader, receptive_field)\n",
    "            val_losses += [val_l]\n",
    "            cliq_dists += [cliq_dist]\n",
    "            if epoch>30:\n",
    "                if (val_losses[-1] > val_losses[-2]) and (val_losses[-1] > val_losses[-3]) and (epoch>validation_timeout):\n",
    "                    print(\"Converged!\")\n",
    "                    if val_losses[-2] < val_losses[-3]:\n",
    "                        print(\"Best model epoch: \", epoch-15)\n",
    "                        file_name = save_path+ str(dataset_name)+ '/' + str(net)+ '_'+ str(epoch-15)+'_samples.pt'\n",
    "                        net.load_state_dict(torch.load(file_name), strict=False)\n",
    "                    else:\n",
    "                        print(\"Best model epoch: \", epoch-30)\n",
    "                        file_name = save_path+ str(dataset_name)+ '/' + str(net)+ '_'+ str(epoch-30)+'_samples.pt'\n",
    "                        net.load_state_dict(torch.load(file_name), strict=False)\n",
    "                    break\n",
    "                if epoch%15==0:\n",
    "                    file_name = save_path+ str(dataset_name)+ '/' + str(net)+ '_'+ str(epoch)+'_samples.pt'\n",
    "                    print(\"file_name: \", file_name)\n",
    "                    torch.save({file_name : net.state_dict()},  file_name)\n",
    "\n",
    "        if epoch % lr_decay_step_size == 0:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                        param_group['lr'] = lr_decay_factor * param_group['lr']\n",
    "        net.train()\n",
    "        for data in train_loader:\n",
    "            count += 1 \n",
    "            optimizer.zero_grad(), \n",
    "            data = data.to(device)\n",
    "            data_prime = get_diracs(data, 1, sparse = True, effective_volume_range=0.15, receptive_field = receptive_field)\n",
    "\n",
    "            data = data.to('cpu')\n",
    "            data_prime = data_prime.to(device)\n",
    "            retdict = net(data_prime, edge_drop_p, penalty_coeff)\n",
    "            \n",
    "            for key,val in retdict.items():\n",
    "                if \"sequence\" in val[1]:\n",
    "                    if key in totalretdict:\n",
    "                        totalretdict[key][0] += val[0].item()\n",
    "                    else:\n",
    "                        totalretdict[key] = [val[0].item(),val[1]]\n",
    "\n",
    "            if epoch > 2:\n",
    "                    retdict[\"loss\"][0].backward()\n",
    "\n",
    "                    torch.nn.utils.clip_grad_norm_(net.parameters(),1)\n",
    "                    optimizer.step()\n",
    "                    del(retdict)\n",
    "\n",
    "            del data_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data = testdata\n",
    "test_loader = DataLoader(test_data, batch-size, shuffle=False)\n",
    "net.to(device)\n",
    "count = 1\n",
    "net.eval()\n",
    "\n",
    "gnn_nodes = []\n",
    "gnn_edges = []\n",
    "gnn_sets = {}\n",
    "#number of random samples\n",
    "max_samples = 1\n",
    "gnn_times = []\n",
    "num_samples = max_samples\n",
    "t_start = time.time()\n",
    "\n",
    "\n",
    "for data in test_loader:\n",
    "    num_graphs = data.batch.max().item()+1\n",
    "    bestset = {}\n",
    "    bestedges = np.zeros((num_graphs))\n",
    "    maxset = np.zeros((num_graphs))\n",
    "    \n",
    "    #generate samples\n",
    "    total_samples = []\n",
    "    for graph in range(num_graphs):\n",
    "        curr_inds = (data.batch==graph)\n",
    "        g_size = curr_inds.sum().item()\n",
    "        if max_samples <= g_size: \n",
    "            samples = np.random.choice(curr_inds.sum().item(),max_samples, replace=False)\n",
    "        else:\n",
    "            samples = np.random.choice(curr_inds.sum().item(),max_samples, replace=True)\n",
    "\n",
    "        total_samples +=[samples]\n",
    "\n",
    "    data = data.to(device)\n",
    "    t_0 = time.time()\n",
    "    for k in range(num_samples):\n",
    "        t_datanet_0 = time.time()\n",
    "        data_prime = get_diracs(data.to(device), 1, sparse = True, effective_volume_range=0.15, receptive_field = 7)\n",
    "        print(\"Number of nodes: \", data_prime.x.shape[0])\n",
    "        initial_values = data_prime.x.detach()\n",
    "        data_prime.x = torch.zeros_like(data_prime.x)\n",
    "        g_offset = 0\n",
    "        for graph in range(num_graphs):\n",
    "            curr_inds = (data_prime.batch==graph)\n",
    "            g_size = curr_inds.sum().item()\n",
    "            graph_x = data_prime.x[curr_inds]\n",
    "            data_prime.x[total_samples[graph][k] + g_offset]=1.\n",
    "            g_offset += g_size\n",
    "       \n",
    "        #forward_pass  \n",
    "        ret_dict = net(data_prime)\n",
    "        \n",
    "        #derandomize\n",
    "        t_datanet_1 = time.time() - t_datanet_0\n",
    "        print(\"data prep and fp: \", t_datanet_1)\n",
    "        t_derand_0 = time.time()\n",
    "        _, set_edges, set_cardinality = derandomize_clique_final_speed(data_prime,(ret_dict[\"output\"][0]), weight_factor =0.,draw=False, beam = 1)\n",
    "        t_derand_1 = time.time() - t_derand_0\n",
    "        print(\"Derandomization time: \", t_derand_1)\n",
    "        for j in range(num_graphs):\n",
    "            indices = (data.batch == j)\n",
    "            if (set_cardinality[j]>maxset[j]):\n",
    "                    maxset[j] = set_cardinality[j].item()\n",
    "                    bestset[str(j)] = sets[indices].cpu()\n",
    "                    bestedges[j] = set_edges[j].item()\n",
    "\n",
    "    t_1 = time.time()-t_0\n",
    "    print(\"Current batch: \", count)\n",
    "    print(\"Time so far: \", time.time()-t_0)\n",
    "    gnn_sets[str(count)] = bestset\n",
    "    \n",
    "    gnn_nodes += [maxset]\n",
    "    gnn_edges += [bestedges]\n",
    "    gnn_times += [t_1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    count += 1\n",
    "t_1 = time.time()\n",
    "total_time = t_1 - t_start\n",
    "print(\"Average time per graph: \", total_time/(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get clique numbers\n",
    "test_data_clique = []\n",
    "for data in testdata:\n",
    "    my_graph = to_networkx(Data(x=data.x, edge_index = data.edge_index)).to_undirected()\n",
    "    print(my_graph)\n",
    "    cliqno, _ = solve_gurobi_maxclique(my_graph, 100000)\n",
    "    data.clique_number = cliqno\n",
    "    test_data_clique += [data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten the output list\n",
    "flat_list = [item for sublist in gnn_edges for item in sublist]\n",
    "for k in range(len(flat_list)):\n",
    "    flat_list[k] = flat_list[k].item()\n",
    "gnn_edges = (flat_list)\n",
    "\n",
    "flat_list = [item for sublist in gnn_nodes for item in sublist]\n",
    "for k in range(len(flat_list)):\n",
    "    flat_list[k] = flat_list[k].item()\n",
    "gnn_nodes = (flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = test_data_clique\n",
    "ratios = [gnn_nodes[i]/tests[i].clique_number for i in range(len(tests))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean ratio: {(np.array(ratios)).mean()} +/-  {(np.array(ratios)).std()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Stalence)",
   "language": "python",
   "name": "stalence"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
