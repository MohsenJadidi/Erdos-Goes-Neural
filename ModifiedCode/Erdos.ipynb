{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Erdos.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPwKKEmHMJn1oIw2GnwaPSi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohsenJadidi/Erdos-Goes-Neural/blob/main/ModifiedCode/Erdos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6rldKktrQgQ",
        "outputId": "0bf46240-eb0f-4159-c310-417c3f9673a7"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive/')\r\n",
        "\r\n",
        "import sys\r\n",
        "sys.path.append('/content/drive/My Drive/ISLR Course - Fall 2020/Project')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXR2JFkArVDW",
        "outputId": "c44e0c47-6371-4e4f-8085-7d0c6a292b00"
      },
      "source": [
        "!pip install torch_scatter\r\n",
        "!pip install torch_sparse\r\n",
        "!pip install torch_geometric\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch_scatter\n",
            "  Downloading https://files.pythonhosted.org/packages/01/45/cd93ed3227248773ba8bc4b3beaa04e8bddb127a793a41bad875369951b3/torch_scatter-2.0.5.tar.gz\n",
            "Building wheels for collected packages: torch-scatter\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMQuP5ut4An3",
        "outputId": "6cae71d6-ecc0-4481-ba5e-721c3c7e694a"
      },
      "source": [
        "!apt-get install python3-dev graphviz libgraphviz-dev pkg-config\r\n",
        "!pip install graphviz\r\n",
        "!pip install pygraphviz\r\n",
        "\r\n",
        "!pip install visdom\r\n",
        "!pip install GPUtill==1.4.0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "pkg-config is already the newest version (0.29.1-0ubuntu2).\n",
            "graphviz is already the newest version (2.40.1-2).\n",
            "libgraphviz-dev is already the newest version (2.40.1-2).\n",
            "python3-dev is already the newest version (3.6.7-1~18.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 11 not upgraded.\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
            "Requirement already satisfied: pygraphviz in /usr/local/lib/python3.7/dist-packages (1.7)\n",
            "Requirement already satisfied: visdom in /usr/local/lib/python3.7/dist-packages (0.1.8.9)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.7/dist-packages (from visdom) (1.19.5)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from visdom) (22.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from visdom) (2.23.0)\n",
            "Requirement already satisfied: torchfile in /usr/local/lib/python3.7/dist-packages (from visdom) (0.1.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.7/dist-packages (from visdom) (0.57.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from visdom) (1.4.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from visdom) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from visdom) (1.15.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.7/dist-packages (from visdom) (5.1.1)\n",
            "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.7/dist-packages (from visdom) (1.28)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (2020.12.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.7/dist-packages (from jsonpatch->visdom) (2.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement GPUtill==1.4.0 (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for GPUtill==1.4.0\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxezMd9Hq-tg"
      },
      "source": [
        "import os\r\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\r\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"#imports\r\n",
        "#imports\r\n",
        "import torch\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "from torch.nn import Linear\r\n",
        "\r\n",
        "\r\n",
        "#from kernel.datasets import get_dataset\r\n",
        "from itertools import product\r\n",
        "\r\n",
        "import time\r\n",
        "\r\n",
        "\r\n",
        "from torch import tensor\r\n",
        "from torch.optim import Adam\r\n",
        "from torch.optim import SGD\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "from torch_geometric.data import DataLoader, DenseDataLoader as DenseLoader\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "from math import ceil\r\n",
        "\r\n",
        "from torch.nn import Linear\r\n",
        "\r\n",
        "from torch.distributions import categorical\r\n",
        "from torch.distributions import Bernoulli\r\n",
        "\r\n",
        "import torch.nn\r\n",
        "\r\n",
        "%matplotlib inline\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "\r\n",
        "#import pygraphviz as pgv\r\n",
        "\r\n",
        "from torch_geometric.utils import convert as cnv\r\n",
        "from torch_geometric.utils import sparse as sp\r\n",
        "\r\n",
        "from torch_geometric.data import Data\r\n",
        "\r\n",
        "#import pygraphviz as pgv\r\n",
        "\r\n",
        "from networkx.drawing.nx_agraph import graphviz_layout\r\n",
        "\r\n",
        "import networkx as nx\r\n",
        "\r\n",
        "\r\n",
        "from torch.utils.data.sampler import RandomSampler\r\n",
        "\r\n",
        "from torch.nn.functional import gumbel_softmax\r\n",
        "\r\n",
        "\r\n",
        "from torch.distributions import relaxed_categorical\r\n",
        "\r\n",
        "#import myfuncs #-----------------\r\n",
        "\r\n",
        "#from tensorboardX import SummaryWriter\r\n",
        "\r\n",
        "from torch_geometric.nn.inits import uniform\r\n",
        "from torch_geometric.nn.inits import glorot, zeros\r\n",
        "\r\n",
        "\r\n",
        "from torch.nn import Parameter\r\n",
        "\r\n",
        "from torch.nn import Sequential as Seq, Linear, ReLU\r\n",
        "from torch_geometric.nn import MessagePassing\r\n",
        "\r\n",
        "from torch_geometric.utils import degree\r\n",
        "\r\n",
        "from torch_geometric.nn import GINConv, GATConv, global_mean_pool, NNConv, GCNConv\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "from torch.nn import Parameter\r\n",
        "\r\n",
        "from torch.nn import Sequential as Seq, Linear, ReLU, LeakyReLU\r\n",
        "from torch_geometric.nn import MessagePassing\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.nn import Linear, Sequential, ReLU, BatchNorm1d as BN\r\n",
        "from torch_geometric.nn import GINConv, global_mean_pool\r\n",
        "\r\n",
        "\r\n",
        "from torch_geometric.data import Batch \r\n",
        "\r\n",
        "\r\n",
        "from torch_scatter import scatter_min, scatter_max, scatter_add, scatter_mean\r\n",
        "\r\n",
        "\r\n",
        "from torch import autograd\r\n",
        "\r\n",
        "from torch_geometric.utils import to_dense_batch, to_dense_adj\r\n",
        "\r\n",
        "\r\n",
        "from torch_geometric.utils import softmax, add_self_loops, remove_self_loops, segregate_self_loops, remove_isolated_nodes, contains_isolated_nodes, add_remaining_self_loops\r\n",
        "\r\n",
        "\r\n",
        "from torch_geometric.utils import dropout_adj, to_undirected, to_networkx\r\n",
        "from torch_geometric.utils import is_undirected\r\n",
        "\r\n",
        "from cut_utils import get_diracs\r\n",
        "\r\n",
        "import scipy\r\n",
        "\r\n",
        "import scipy.io\r\n",
        "\r\n",
        "from matplotlib.lines import Line2D\r\n",
        "\r\n",
        "from torch_geometric.utils.convert import from_scipy_sparse_matrix\r\n",
        "\r\n",
        "# import GPUtil #-----------!!!!!????\r\n",
        "\r\n",
        "\r\n",
        "from networkx.algorithms.approximation import max_clique\r\n",
        "\r\n",
        "\r\n",
        "import pickle\r\n",
        "\r\n",
        "\r\n",
        "from torch_geometric.nn import SplineConv, global_mean_pool, DataParallel\r\n",
        "\r\n",
        "from torch_geometric.data import DataListLoader\r\n",
        "from random import shuffle\r\n",
        "\r\n",
        "\r\n",
        "from networkx.algorithms.approximation import max_clique\r\n",
        "from networkx.algorithms import graph_clique_number\r\n",
        "\r\n",
        "from networkx.algorithms import find_cliques\r\n",
        "\r\n",
        "from torch_geometric.nn.norm import graph_size_norm"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wmkSoPG7MKP"
      },
      "source": [
        "import visdom \r\n",
        "from visdom import Visdom \r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def plot_grad_flow( named_parameters):\r\n",
        "    ave_grads = []\r\n",
        "    layers = []\r\n",
        "    for n, p in named_parameters:\r\n",
        "        if(p.requires_grad) and (\"bias\" not in n):\r\n",
        "            layers.append(n)\r\n",
        "            ave_grads.append(p.grad.abs().mean())\r\n",
        "            \r\n",
        "    plt.plot(ave_grads, alpha=0.3, color=\"b\")\r\n",
        "    plt.yscale('log')\r\n",
        "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\r\n",
        "    plt.xlim(xmin=0, xmax=len(ave_grads))\r\n",
        "    plt.xlabel(\"Layers\")\r\n",
        "    plt.ylabel(\"average gradient\")\r\n",
        "    plt.title(\"Gradient flow\")\r\n",
        "    plt.grid(True)\r\n",
        "    \r\n",
        "\r\n",
        "class VisdomLinePlotter(object):\r\n",
        "    \"\"\"Plots to Visdom\"\"\"\r\n",
        "    def __init__(self, env_name='main'):\r\n",
        "        self.viz = Visdom(port=8097)\r\n",
        "        self.env = env_name\r\n",
        "        self.plots = {}\r\n",
        "    def plot(self, var_name, split_name, title_name, x, y):\r\n",
        "        if title_name not in self.plots:\r\n",
        "            self.plots[title_name] = self.viz.line(X=np.array([x,x]), Y=np.array([y,y]), env=self.env, opts=dict(\r\n",
        "                legend=[split_name],\r\n",
        "                title=title_name,\r\n",
        "                xlabel='Epochs',\r\n",
        "                ylabel=var_name\r\n",
        "            ))\r\n",
        "        else:\r\n",
        "            self.viz.line(X=np.array([x]), Y=np.array([y]), env=self.env, win=self.plots[title_name], name=split_name, update = 'append')\r\n",
        "    \r\n",
        "\r\n",
        "    def histog(self,title_name,vals):\r\n",
        "        if title_name not in self.plots:\r\n",
        "            self.plots[title_name] = self.viz.histogram(X=vals,env=self.env,opts=dict(title=title_name,numbins=20))\r\n",
        "        else:\r\n",
        "            self.viz.histogram(X=vals,env=self.env,win=self.plots[title_name],opts=dict(title=title_name,numbins=20, update = 'replace'))\r\n",
        "  \r\n",
        "    def gradflow(self, model, title_name):\r\n",
        "                    title_name = \"Gradflow\"\r\n",
        "                    layers = []\r\n",
        "                    ave_grads = []\r\n",
        "                    for n, p in net.named_parameters():\r\n",
        "                            if(p.requires_grad) and (\"bias\" not in n):\r\n",
        "                                     layers.append(str(n))\r\n",
        "                                     ave_grads.append(p.grad.abs().mean().cpu().numpy())\r\n",
        "                    if title_name not in self.plots:\r\n",
        "                        self.plots[title_name]= self.viz.line(X= list(range(len(layers))),Y= np.array(ave_grads),  env = self.env,  opts=dict(fillarea=True, title = title_name, xtick = True,\r\n",
        "        xtickmin=0, xtickmax=len(layers), xtickvals = list(range(len(layers))), xtickstep=1/len(layers), xticklabels = layers) )    \r\n",
        "                    else:\r\n",
        "                        self.viz.line(X= list(range(len(layers))), Y=ave_grads, env = self.env ,win=self.plots[title_name], opts=dict(fillarea=True, title = title_name, xtick = True,\r\n",
        "        xtickmin=0, xtickmax=len(layers), xtickvals = list(range(len(layers))), xtickstep=1/len(layers), xticklabels = layers, update=\"append\"))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84TPany58um-"
      },
      "source": [
        "class GCNConv2(MessagePassing):\r\n",
        "    r\"\"\"The graph convolutional operator from the `\"Semi-supervised\r\n",
        "    Classfication with Graph Convolutional Networks\"\r\n",
        "    <https://arxiv.org/abs/1609.02907>`_ paper\r\n",
        "\r\n",
        "    .. math::\r\n",
        "        \\mathbf{X}^{\\prime} = \\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\r\n",
        "        \\mathbf{\\hat{D}}^{-1/2} \\mathbf{X} \\mathbf{\\Theta},\r\n",
        "\r\n",
        "    where :math:`\\mathbf{\\hat{A}} = \\mathbf{A} + \\mathbf{I}` denotes the\r\n",
        "    adjacency matrix with inserted self-loops and\r\n",
        "    :math:`\\hat{D}_{ii} = \\sum_{j=0} \\hat{A}_{ij}` its diagonal degree matrix.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        in_channels (int): Size of each input sample.\r\n",
        "        out_channels (int): Size of each output sample.\r\n",
        "        improved (bool, optional): If set to :obj:`True`, the layer computes\r\n",
        "            :math:`\\mathbf{\\hat{A}}` as :math:`\\mathbf{A} + 2\\mathbf{I}`.\r\n",
        "            (default: :obj:`False`)\r\n",
        "        cached (bool, optional): If set to :obj:`True`, the layer will cache\r\n",
        "            the computation of :math:`{\\left(\\mathbf{\\hat{D}}^{-1/2}\r\n",
        "            \\mathbf{\\hat{A}} \\mathbf{\\hat{D}}^{-1/2} \\right)}`.\r\n",
        "            (default: :obj:`False`)\r\n",
        "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\r\n",
        "            an additive bias. (default: :obj:`True`)\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self,\r\n",
        "                 in_channels,\r\n",
        "                 out_channels,\r\n",
        "                 improved=False,\r\n",
        "                 cached=False,\r\n",
        "                 bias=True):\r\n",
        "        super(GCNConv2, self).__init__('add')\r\n",
        "\r\n",
        "        self.in_channels = in_channels\r\n",
        "        self.out_channels = out_channels\r\n",
        "        self.improved = improved\r\n",
        "        self.cached = cached\r\n",
        "        self.cached_result = None\r\n",
        "\r\n",
        "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\r\n",
        "\r\n",
        "        if bias:\r\n",
        "            self.bias = Parameter(torch.Tensor(out_channels))\r\n",
        "        else:\r\n",
        "            self.register_parameter('bias', None)\r\n",
        "\r\n",
        "        self.reset_parameters()\r\n",
        "\r\n",
        "    def reset_parameters(self):\r\n",
        "        glorot(self.weight)\r\n",
        "        zeros(self.bias)\r\n",
        "        self.cached_result = None\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def norm(edge_index, num_nodes, edge_weight, improved=False, dtype=None):\r\n",
        "        if edge_weight is None:\r\n",
        "            edge_weight = torch.ones((edge_index.size(1), ),\r\n",
        "                                     dtype=dtype,\r\n",
        "                                     device=edge_index.device)\r\n",
        "        edge_weight = edge_weight.view(-1)\r\n",
        "        assert edge_weight.size(0) == edge_index.size(1)\r\n",
        "\r\n",
        "#         print(\"before: \")\r\n",
        "#         print(edge_weight.shape)\r\n",
        "#         print(edge_index.shape)\r\n",
        "        \r\n",
        "        #edge_index, edge_weight = remove_self_loops(edge_index, edge_weight)\r\n",
        "        #edge_index = add_self_loops(edge_index, num_nodes)\r\n",
        "#         loop_weight = torch.full((num_nodes, ),\r\n",
        "#                                  1 if not improved else 2,\r\n",
        "#                                  dtype=edge_weight.dtype,\r\n",
        "#                                  device=edge_weight.device)\r\n",
        "        #edge_weight = torch.cat([edge_weight, loop_weight], dim=0)\r\n",
        "#         print(\"after: \")\r\n",
        "#         print(edge_weight.shape)\r\n",
        "#         print(edge_index.shape)\r\n",
        "        row, col = edge_index\r\n",
        "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\r\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\r\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\r\n",
        "\r\n",
        "        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\r\n",
        "\r\n",
        "    def forward(self, x, edge_index, edge_weight=None):\r\n",
        "        \"\"\"\"\"\"\r\n",
        "        x = torch.matmul(x, self.weight)\r\n",
        "\r\n",
        "        if not self.cached or self.cached_result is None:\r\n",
        "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight,\r\n",
        "                                         self.improved, x.dtype)\r\n",
        "            self.cached_result = edge_index, norm\r\n",
        "        edge_index, norm = self.cached_result\r\n",
        "\r\n",
        "        return self.propagate(edge_index, x=x, norm=norm)\r\n",
        "\r\n",
        "    def message(self, x_j, norm):\r\n",
        "        return norm.view(-1, 1) * x_j\r\n",
        "\r\n",
        "    def update(self, aggr_out):\r\n",
        "        if self.bias is not None:\r\n",
        "            aggr_out = aggr_out + self.bias\r\n",
        "        return aggr_out\r\n",
        "\r\n",
        "    def __repr__(self):\r\n",
        "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\r\n",
        "                                   self.out_channels)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nMuqavr8x1V"
      },
      "source": [
        "def propagate(x, edge_index):\r\n",
        "        row, col = edge_index\r\n",
        "        out = scatter_add(x[col], row, dim=0)\r\n",
        "        return out\r\n",
        "\r\n",
        "def get_mask(x, edge_index, hops):\r\n",
        "        for k in range(hops):\r\n",
        "            x = propagate(x, edge_index)\r\n",
        "        mask = (x>0).float()\r\n",
        "        return mask\r\n",
        "\r\n",
        "\r\n",
        "def total_var(x, edge_index, batch, undirected = True):\r\n",
        "        row, col = edge_index\r\n",
        "        if undirected:\r\n",
        "            tv = (torch.abs(x[row]-x[col])) * 0.5\r\n",
        "        else:\r\n",
        "            tv = (torch.abs(x[row]-x[col]))\r\n",
        "        \r\n",
        "        tv = scatter_add(tv, batch[row], dim=0)\r\n",
        "        return  tv"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMFYTU6H-c6j"
      },
      "source": [
        "#from pytorch_memlab import profile, mem_reporter\r\n",
        "#@profile\r\n",
        "class cliqueMPNN_hindsight_earlyGAT(torch.nn.Module):\r\n",
        "    def __init__(self, dataset, num_layers, hidden1, hidden2, deltas, elasticity=0.01, num_iterations = 30):\r\n",
        "        super(cliqueMPNN_hindsight_earlyGAT, self).__init__()\r\n",
        "        self.hidden1 = hidden1\r\n",
        "        self.hidden2 = hidden2\r\n",
        "        self.momentum = 0.1\r\n",
        "        #self.nns = Sequential(\r\n",
        "                #Linear(2*hidden, hidden*hidden),\r\n",
        "                #LeakyReLU(0.1))\r\n",
        "        self.num_iterations = num_iterations\r\n",
        "        self.convs = torch.nn.ModuleList()\r\n",
        "        self.deltas = deltas\r\n",
        "        self.numlayers = num_layers\r\n",
        "        self.elasticity = elasticity\r\n",
        "        self.heads = 8\r\n",
        "        self.concat = True\r\n",
        "        \r\n",
        "        self.bns = torch.nn.ModuleList()\r\n",
        "        for i in range(num_layers-1):\r\n",
        "            self.bns.append(BN(self.heads*self.hidden1, momentum=self.momentum))\r\n",
        "\r\n",
        "        \r\n",
        "#         self.nns = torch.nn.ModuleList()\r\n",
        "#         for i in range(num_layers-1):\r\n",
        "#             self.nns.append(Sequential(\r\n",
        "#                 Linear(2*hidden, hidden*hidden),\r\n",
        "#                 LeakyReLU(0.1)))\r\n",
        "        \r\n",
        "        self.convs = torch.nn.ModuleList()        \r\n",
        "        for i in range(num_layers - 1):\r\n",
        "                #self.convs.append(GATAConv(self.heads*self.hidden1, self.hidden1, concat=self.concat ,heads=self.heads))\r\n",
        "                self.convs.append(GINConv(Sequential(\r\n",
        "            Linear( self.heads*self.hidden1,  self.heads*self.hidden1),\r\n",
        "            ReLU(),\r\n",
        "            Linear( self.heads*self.hidden1,  self.heads*self.hidden1),\r\n",
        "            ReLU(),\r\n",
        "            BN(self.heads*self.hidden1, momentum=self.momentum),\r\n",
        "        ),train_eps=True))\r\n",
        "                #self.convs.append(GCNConv2(self.heads*self.hidden1, self.heads*self.hidden1))\r\n",
        "        self.bn1 = BN(self.heads*self.hidden1)\r\n",
        "        #self.conv1 = GATAConv(self.hidden2, self.hidden1, concat=self.concat ,heads=self.heads)\r\n",
        "       \r\n",
        "        self.conv1 = GINConv(Sequential(Linear(self.hidden2,  self.heads*self.hidden1),\r\n",
        "            ReLU(),\r\n",
        "            Linear( self.heads*self.hidden1,  self.heads*self.hidden1),\r\n",
        "            ReLU(),\r\n",
        "            BN(self.heads*self.hidden1, momentum=self.momentum),\r\n",
        "        ),train_eps=True)\r\n",
        "\r\n",
        "        if self.concat:\r\n",
        "            self.lin1 = Linear(self.heads*self.hidden1, self.hidden1)\r\n",
        "        else:\r\n",
        "            self.lin1 = Linear(self.hidden1, self.hidden1)\r\n",
        "\r\n",
        "        #self.bn2 = BN(self.hidden1, momentum=self.momentum)\r\n",
        "        self.lin2 = Linear(self.hidden1, 1)\r\n",
        "        self.gnorm = graph_size_norm.GraphSizeNorm()\r\n",
        "\r\n",
        "                    \r\n",
        "\r\n",
        "\r\n",
        "    def reset_parameters(self):\r\n",
        "        self.conv1.reset_parameters()\r\n",
        "        #self.conv2.reset_parameters()\r\n",
        "        \r\n",
        "        for conv in self.convs:\r\n",
        "            conv.reset_parameters()\r\n",
        "            \r\n",
        "        for bn in self.bns:\r\n",
        "            bn.reset_parameters()\r\n",
        "            \r\n",
        "        self.bn1.reset_parameters()\r\n",
        "        self.lin1.reset_parameters()\r\n",
        "        #self.bn2.reset_parameters()\r\n",
        "        self.lin2.reset_parameters()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, data, edge_dropout = None, penalty_coefficient = 0.25):\r\n",
        "        x = data.x\r\n",
        "        edge_index = data.edge_index\r\n",
        "        batch = data.batch\r\n",
        "        num_graphs = batch.max().item() + 1\r\n",
        "        row, col = edge_index     \r\n",
        "        total_num_edges = edge_index.shape[1]\r\n",
        "        N_size = x.shape[0]\r\n",
        "\r\n",
        "        \r\n",
        "        if edge_dropout is not None:\r\n",
        "            edge_index = dropout_adj(edge_index, edge_attr = (torch.ones(edge_index.shape[1], device=device)).long(), p = edge_dropout, force_undirected=True)[0]\r\n",
        "            edge_index = add_remaining_self_loops(edge_index, num_nodes = batch.shape[0])[0]\r\n",
        "                \r\n",
        "        reduced_num_edges = edge_index.shape[1]\r\n",
        "        current_edge_percentage = (reduced_num_edges/total_num_edges)\r\n",
        "        no_loop_index,_ = remove_self_loops(edge_index)  \r\n",
        "        no_loop_row, no_loop_col = no_loop_index\r\n",
        "\r\n",
        "        xinit= x.clone()\r\n",
        "\r\n",
        "        #x = torch.cat([x.unsqueeze(-1),torch.rand((x.shape[0],self.hidden2-1),device=device)], dim=1)\r\n",
        "        #x = torch.rand((x.shape[0],self.hidden2),device=device)\r\n",
        "        #\r\n",
        "        x = x.unsqueeze(-1)\r\n",
        "        #mask = get_mask(x,edge_index,1).to(x.dtype)\r\n",
        "        \r\n",
        "#         print(\"xbefore:\",x.shape)\r\n",
        "\r\n",
        "        mask = get_mask(x,edge_index,1).to(x.dtype)\r\n",
        "        x = F.leaky_relu(self.conv1(x, edge_index))# +x\r\n",
        "        x = x*mask\r\n",
        "\r\n",
        "#         print(\"xshape:\",x.shape)\r\n",
        "#         print(\"mask: \", mask.shape)\r\n",
        "        \r\n",
        "        #x = F.leaky_relu(self.conv1(x, edge_index)) + x\r\n",
        "        #x = self.conv1(x, edge_index)\r\n",
        "        #print(x.shape)\r\n",
        "        x = self.gnorm(x)\r\n",
        "        x = self.bn1(x)\r\n",
        "        \r\n",
        "\r\n",
        "        \r\n",
        "        #print(x.shape)\r\n",
        "            \r\n",
        "        for conv, bn in zip(self.convs, self.bns):\r\n",
        "            if(x.dim()>1):\r\n",
        "                x =  x+F.leaky_relu(conv(x, edge_index))\r\n",
        "                mask = get_mask(mask,edge_index,1).to(x.dtype)\r\n",
        "                x = x*mask\r\n",
        "                x = self.gnorm(x)\r\n",
        "                x = bn(x)\r\n",
        "\r\n",
        "\r\n",
        "#         x = self.conv2(x, edge_index)\r\n",
        "#         mask = get_mask(mask,edge_index,1).to(x.dtype)\r\n",
        "#         x = x*mask\r\n",
        "        xpostconvs = x.detach()\r\n",
        "        #\r\n",
        "        x = F.leaky_relu(self.lin1(x)) \r\n",
        "        x = x*mask\r\n",
        "        #x = self.gnorm(x)\r\n",
        "        #x = self.bn2(x)\r\n",
        "\r\n",
        "\r\n",
        "        xpostlin1 = x.detach()\r\n",
        "        #x = F.dropout(x, p=0.0, training=self.training)\r\n",
        "        x = F.leaky_relu(self.lin2(x)) \r\n",
        "       # x = self.gnorm(x)\r\n",
        "\r\n",
        "        x = x*mask\r\n",
        "\r\n",
        "\r\n",
        "        #xprethresh = x.detach()\r\n",
        "\r\n",
        "        #x = x/scatter_add(torch.abs(x), batch,0)[batch]\r\n",
        "\r\n",
        "        \r\n",
        "        #calculate min and max\r\n",
        "        batch_max = scatter_max(x, batch, 0, dim_size= N_size)[0]\r\n",
        "        batch_max = torch.index_select(batch_max, 0, batch)        \r\n",
        "        batch_min = scatter_min(x, batch, 0, dim_size= N_size)[0]\r\n",
        "        batch_min = torch.index_select(batch_min, 0, batch)\r\n",
        "\r\n",
        "                \r\n",
        "        x = (x-batch_min)/(batch_max+1e-6-batch_min)\r\n",
        "        #x = x*mask + mask*1e-6\r\n",
        "        \r\n",
        "\r\n",
        "        probs=x\r\n",
        "        \r\n",
        "        #print(probs.shape)\r\n",
        "        \r\n",
        "        x2 = x.detach()              \r\n",
        "        deg = degree(row).unsqueeze(-1) \r\n",
        "        totalvol = scatter_add(deg.detach()*torch.ones_like(x, device=device), batch, 0)+1e-6\r\n",
        "        totalcard = scatter_add(torch.ones_like(x, device=device), batch, 0)+1e-6\r\n",
        "        \r\n",
        "                \r\n",
        "        #volume within receptive field\r\n",
        "        #recvol_hard = scatter_add(deg*mask.float(), batch, 0, dim_size = num_graphs)+1e-6 \r\n",
        "        #reccard_hard = scatter_add(mask.float(), batch, 0, dim_size = num_graphs)+1e-6 \r\n",
        "        \r\n",
        "        #assert recvol_hard.mean()/totalvol.mean() <=1, \"Something went wrong! Receptive field is larger than total volume.\"\r\n",
        "\r\n",
        "        \r\n",
        "        \r\n",
        "        x2 =  ((x2 - torch.rand_like(x, device = device))>0).float()\r\n",
        "        \r\n",
        "        vol_1 = scatter_add(probs*deg, batch, 0)+1e-6\r\n",
        "        card_1 = scatter_add(probs, batch,0)            \r\n",
        "        #rec_field = scatter_add(mask, batch, 0)+1e-6\r\n",
        "        set_size = scatter_add(x2, batch, 0)\r\n",
        "#         tv_hard = total_var(x2, edge_index, batch)\r\n",
        "        vol_hard = scatter_add(deg*x2, batch, 0, dim_size = batch.max().item()+1)+1e-6 \r\n",
        "#         conduct_hard = tv_hard/vol_hard\r\n",
        "            \r\n",
        "#         rec_field_ratio = set_size/rec_field\r\n",
        "#         rec_field_volratio = vol_hard/recvol_hard\r\n",
        "        total_vol_ratio = vol_hard/totalvol\r\n",
        "        \r\n",
        "        #volume within receptive field\r\n",
        "        #recvol_hard = scatter_add(deg*mask.float(), batch, 0, dim_size = num_graphs)+1e-6 \r\n",
        "        #reccard_hard = scatter_add(mask.float(), batch, 0, dim_size = num_graphs)+1e-6 \r\n",
        "        \r\n",
        "        \r\n",
        "        #calculating the terms for the expected distance between clique and graph\r\n",
        "        pairwise_prodsums = torch.zeros(num_graphs, device = device)\r\n",
        "        for graph in range(num_graphs):\r\n",
        "            batch_graph = (batch==graph)\r\n",
        "            pairwise_prodsums[graph] = (torch.conv1d(probs[batch_graph].unsqueeze(-1), probs[batch_graph].unsqueeze(-1))).sum()/2\r\n",
        "        \r\n",
        "        \r\n",
        "        ##############CAREFUL\r\n",
        "        self_sums = scatter_add((probs*probs), batch, 0, dim_size = num_graphs)/2.\r\n",
        "\r\n",
        "        #self_sums = scatter_add((probs*probs), batch, 0, dim_size = num_graphs)/1.\r\n",
        "        \r\n",
        "        #expected_weight_G = scatter_add(probs[no_loop_row]*probs[no_loop_col], batch[no_loop_row], 0, dim_size = num_graphs)/2\r\n",
        "        \r\n",
        "        expected_weight_G = scatter_add(probs[no_loop_row]*probs[no_loop_col], batch[no_loop_row], 0, dim_size = num_graphs)/2.\r\n",
        "        expected_clique_weight = (pairwise_prodsums.unsqueeze(-1) - self_sums)/1.\r\n",
        "        expected_distance = (expected_clique_weight - expected_weight_G)\r\n",
        "        \r\n",
        "        \r\n",
        "        #lambda_factors = (torch.rand((30,1), device=device))*penalty_coefficient-0.10\r\n",
        "        \r\n",
        "        \r\n",
        "        #hindsight = torch.ones_like(lambda_factors)*expected_distance.unsqueeze(-1)*0.5 - lambda_factors*expected_weight_G.unsqueeze(-1)\r\n",
        "        \r\n",
        "\r\n",
        "       # print(expected_clique_weight.shape)\r\n",
        "        #print(self_sums.shape)\r\n",
        "\r\n",
        "       # expected_loss =  #torch.median(hindsight, 1)[0]\r\n",
        "        #print(expected_loss.shape)\r\n",
        "        \r\n",
        "        max_set_weight = (scatter_add(torch.ones_like(x)[no_loop_row], batch[no_loop_row], 0, dim_size = num_graphs)/2).squeeze(-1)\r\n",
        "        \r\n",
        "        #print(\"how many:\", (data.batch==0).sum())\r\n",
        "        \r\n",
        "        set_weight = (scatter_add(x2[no_loop_row]*x2[no_loop_col], batch[no_loop_row], 0, dim_size = num_graphs)/2)+1e-6\r\n",
        "        clique_edges_hard = (set_size*(set_size-1)/2) +1e-6\r\n",
        "        clique_dist_hard = set_weight/clique_edges_hard\r\n",
        "        \r\n",
        "        \r\n",
        "#          print(\"cardinalities: \", cardinalities)\r\n",
        "#         print(\"max_set_weight:\", max_set_weight)\r\n",
        "#        print(\"max possible weight: \", max_possible_weight)\r\n",
        "        #print(max_set_weight.shape)\r\n",
        "        #print(max_possible_weight.shape)\r\n",
        "        cardinalities =  scatter_add(torch.ones_like(data.batch),data.batch,dim=0).float()\r\n",
        "        max_possible_weight = (cardinalities*(cardinalities-1)/2)\r\n",
        "#         diff = (max_possible_weight - max_set_weight) \r\n",
        "        penalty_coefficient =   (max_set_weight/max_possible_weight) * penalty_coefficient\r\n",
        "        #penalty_coefficient =  penalty_coefficient*(clique_dist_hard/(1-clique_dist_hard))\r\n",
        "                \r\n",
        "        #penalty_coefficient[clique_dist_hard>0.95] = 1.\r\n",
        "\r\n",
        "        #print(\"pen coeff:\", penalty_coefficient)\r\n",
        "#         print(\"Diff: \", diff)\r\n",
        "#         print(\"penalties:\", penalties.shape)\r\n",
        "#         print(\"exp dist:\", expected_distance.shape)\r\n",
        "        \r\n",
        "        #assert ((max_possible_weight<max_set_weight).sum())<=1e-6, \"Invalid calculation\"\r\n",
        "        expected_ratio = expected_weight_G/expected_distance\r\n",
        "        expected_ratio = expected_ratio.detach()\r\n",
        "        #penalty_coefficient = expected_ratio*penalty_coefficient\r\n",
        "       # print(expected_ratio.shape)\r\n",
        "                               \r\n",
        "        #expected_loss = (penalty_coefficient)*expected_distance*0.5*expected_weight_G - 0.5*expected_weight_G\r\n",
        "        #expected_loss = expected_clique_weight/expected_weight_G\r\n",
        "        expected_loss = (penalty_coefficient)*expected_distance*0.5 - 0.5*expected_weight_G\r\n",
        "        #expected_loss = (penalty_coefficient)*expected_clique_weight*0.5- 1.*expected_weight_G\r\n",
        "        \r\n",
        "#         print(\"pen coeff: \", penalty_coefficient)\r\n",
        "#         print(\"loss: \", expected_loss)\r\n",
        "    \r\n",
        "    \r\n",
        "#         for iter_graph in range(num_graphs):\r\n",
        "#             if clique_dist_hard[iter_graph]<0.5:\r\n",
        "#                 print(\"problematic graph exp distance: \", expected_distance[iter_graph])\r\n",
        "#                 print(\"problematic graph exp cardinality : \", card_1[iter_graph])\r\n",
        "\r\n",
        "#                 print(\"problematic graph num nodes: \", totalcard[iter_graph])\r\n",
        "                \r\n",
        "        clique_check = ((clique_edges_hard != clique_edges_hard))\r\n",
        "        \r\n",
        "        \r\n",
        "        setedge_check  = ((set_weight != set_weight))\r\n",
        "        \r\n",
        "        \r\n",
        "        \r\n",
        "        assert ((clique_dist_hard>=1.1).sum())<=1e-6, \"Invalid set vol/clique vol ratio.\"\r\n",
        "        \r\n",
        "#        print(expected_loss.shape)\r\n",
        "        \r\n",
        "       # normalize = cardinalities.max()/cardinalities\r\n",
        "        #print(f\"normalize: {normalize}\")\r\n",
        "        loss = expected_loss#*normalize\r\n",
        "        #loss = expected_loss*(1/(clique_dist_hard+0.1))\r\n",
        "\r\n",
        "\r\n",
        "        retdict = {}\r\n",
        "        \r\n",
        "        retdict[\"output\"] = [probs.squeeze(-1),\"hist\"]   #output\r\n",
        "        #retdict[\"clique_check\"] = [clique_edges_hard, \"hist\"]\r\n",
        "        #retdict[\"set_weight_check\"] = [set_weight, \"hist\"]\r\n",
        "        #retdict[\"|Expected_vol - Target|\"]= [targetcheck.squeeze(-1), \"hist\"] #absolute distance from targetvol\r\n",
        "        #retdict[\"Expected_volume\"] = [vol_1.mean(),\"sequence\"] #volume\r\n",
        "        retdict[\"Expected_cardinality\"] = [card_1.mean(),\"sequence\"]\r\n",
        "        retdict[\"Expected_cardinality_hist\"] = [card_1,\"hist\"]\r\n",
        "        retdict[\"losses histogram\"] = [loss.squeeze(-1),\"hist\"]\r\n",
        "        retdict[\"Set sizes\"] = [set_size.squeeze(-1),\"hist\"]\r\n",
        "        retdict[\"volume_hard\"] = [vol_hard.mean(),\"aux\"] #volume2\r\n",
        "        retdict[\"cardinality_hard\"] = [set_size[0],\"sequence\"] #volumeq\r\n",
        "        retdict[\"Expected weight(G)\"]= [expected_weight_G.mean(), \"sequence\"]\r\n",
        "        retdict[\"Expected maximum weight\"] = [expected_clique_weight.mean(),\"sequence\"]\r\n",
        "        retdict[\"Expected distance\"]= [expected_distance.mean(), \"sequence\"]\r\n",
        "        #retdict[\"cut1\"] = [tv.mean(),\"sequence\"] #cut1\r\n",
        "        #retdict[\"cut_hard\"] = [tv_hard.mean(),\"sequence\"] #cut1\r\n",
        "        #retdict[\"Average cardinality ratio of receptive field \"] = [rec_field_ratio.mean(),\"sequence\"] \r\n",
        "        #retdict[\"Recfield volume/Total volume\"] = [recvol_hard.mean()/totalvol.mean(), \"sequence\"]\r\n",
        "        #retdict[\"Average ratio of receptive field volume\"]= [rec_field_volratio.mean(),'sequence']\r\n",
        "        retdict[\"Currvol/Cliquevol\"] = [clique_dist_hard.mean(),'sequence']\r\n",
        "        retdict[\"Currvol/Cliquevol all graphs in batch\"] = [clique_dist_hard.squeeze(-1),'hist']\r\n",
        "        retdict[\"Average ratio of total volume\"]= [total_vol_ratio.mean(),'sequence']\r\n",
        "        #retdict[\"penalty coffs\"] = [penalty_coefficient.squeeze(-1),\"hist\"]\r\n",
        "        retdict[\"cardinalities\"] = [cardinalities.squeeze(-1),\"hist\"]\r\n",
        "        retdict[\"Current edge percentage\"] = [torch.tensor(current_edge_percentage),'sequence']\r\n",
        "        #retdict[\"Clique Weight hard\"] = [clique_edges_hard[0], 'sequence']\r\n",
        "        #retdict[\"Set weight\"] = [set_weight[0], 'sequence']\r\n",
        "        #retdict[\"mask\"] = [mask, \"aux\"] #mask\r\n",
        "        #retdict[\"xinit\"] = [xinit,\"hist\"] #layer input diracs\r\n",
        "        #retdict[\"xpostlin1\"] = [xpostlin1.mean(1),\"hist\"] #after first linear layer\r\n",
        "        #retdict[\"xprethresh\"] = [xprethresh.mean(1),\"hist\"] #pre thresholding activations 195 x 1\r\n",
        "        #retdict[\"xsoftbin\"] = [xsoftbin.mean(1),\"hist\"] #soft binarized output\r\n",
        "        #retdict[\"lossvol\"] = [lossvol.mean(),\"sequence\"] #volume constraint\r\n",
        "        #retdict[\"losscard\"] = [losscard.mean(),\"sequence\"] #cardinality constraint\r\n",
        "        retdict[\"loss\"] = [loss.mean().squeeze(),\"sequence\"] #final loss\r\n",
        "\r\n",
        "        return retdict\r\n",
        "    \r\n",
        "    def __repr__(self):\r\n",
        "        return self.__class__.__name__"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRgZdod5_dh8"
      },
      "source": [
        "def predict(model, data_loader, recfield):\r\n",
        "    model.train()\r\n",
        "    avg_loss = 0\r\n",
        "    avg_cliqdist = 0\r\n",
        "    exp_cardinalities = torch.tensor(0)\r\n",
        "    for data in data_loader:\r\n",
        "        optimizer.zero_grad()\r\n",
        "        data = data.to(device)\r\n",
        "        #print(\"data  batchsum: \",(data.batch==1).sum())\r\n",
        "        data = get_diracs(data, 1, sparse = True, effective_volume_range=0.15, receptive_field = recfield)\r\n",
        "        data = data.to(device)\r\n",
        "        #print(\"data prime batchsum: \",(data.batch==1).sum())\r\n",
        "        retdict = model(data)\r\n",
        "        avg_loss += retdict['loss'][0].item()/len(data_loader)\r\n",
        "        avg_cliqdist += retdict[\"Currvol/Cliquevol\"][0].item()/len(data_loader)\r\n",
        "        exp_cardinalities = [retdict[\"Expected_cardinality_hist\"][0]]\r\n",
        "    #print(\"retdict: \", retdict[\"Expected_cardinality_hist\"][0] )\r\n",
        "    return avg_loss, avg_cliqdist, exp_cardinalities"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "a36HZarZBVDm",
        "outputId": "d79814be-1d03-41d1-9b0d-2fb2a4d7465f"
      },
      "source": [
        "datasets = [\"TWITTER_SNAP\", \"COLLAB\", \"IMDB-BINARY\"]\r\n",
        "dataset_name = datasets[2]\r\n",
        "#datasetname = \"COLLAB_shuffle_1\"\r\n",
        "#datasetname = \"TWITTER_SNAP\"\r\n",
        "#dataset_name = \"IMDB-BINARY\"\r\n",
        "\r\n",
        "if dataset_name == \"TWITTER_SNAP\":\r\n",
        "    stored_dataset = open('datasets/TWITTER_SNAP.p', 'rb')        \r\n",
        "elif dataset_name == \"COLLAB\":\r\n",
        "    stored_dataset = open('datasets/dataset_shuffle_1'+'.p', 'rb')\r\n",
        "elif dataset_name == \"IMDB-BINARY\":\r\n",
        "    stored_dataset = open('/content/drive/My Drive/ISLR Course - Fall 2020/Project/IMDB_BINARY.p', 'rb')\r\n",
        "\r\n",
        "dataset = pickle.load(stored_dataset)\r\n",
        "\r\n",
        "dataset_scale = 1\r\n",
        "# total_samples = int(np.floor(len(dataset)*dataset_scale))\r\n",
        "total_samples = int(np.floor(dataset.len()*dataset_scale))\r\n",
        "# dataset = dataset[:total_samples]\r\n",
        "\r\n",
        "# num_trainpoints = int(np.floor(0.6*len(dataset)))\r\n",
        "# num_valpoints = int(np.floor(num_trainpoints/3))\r\n",
        "# num_testpoints = len(dataset) - (num_trainpoints + num_valpoints)\r\n",
        "\r\n",
        "num_trainpoints = int(np.floor(0.6*dataset.len()))\r\n",
        "num_valpoints = int(np.floor(num_trainpoints/3))\r\n",
        "num_testpoints = dataset.len() - (num_trainpoints + num_valpoints)\r\n",
        "\r\n",
        "traindata= dataset[0:num_trainpoints]\r\n",
        "# valdata = dataset[num_trainpoints:num_trainpoints + num_valpoints]\r\n",
        "# testdata = dataset[num_trainpoints + num_valpoints:]\r\n",
        "\r\n",
        "# batch_size = 32\r\n",
        "\r\n",
        "# train_loader = DataLoader(traindata, batch_size, shuffle=True)\r\n",
        "# test_loader = DataLoader(testdata, batch_size, shuffle=False)\r\n",
        "# val_loader =  DataLoader(valdata, batch_size, shuffle=False)\r\n",
        "\r\n",
        "\r\n",
        "# #set up random seeds \r\n",
        "# torch.manual_seed(1)\r\n",
        "# np.random.seed(2)   \r\n",
        "# torch.backends.cudnn.deterministic = True\r\n",
        "# torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-587aafd6b244>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mnum_testpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_trainpoints\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_valpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mtraindata\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_trainpoints\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;31m# valdata = dataset[num_trainpoints:num_trainpoints + num_valpoints]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# testdata = dataset[num_trainpoints + num_valpoints:]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/data/dataset.py\u001b[0m in \u001b[0;36mindex_select\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/data/dataset.py\u001b[0m in \u001b[0;36mindices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__indices__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__indices__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'TUDataset' object has no attribute '__indices__'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "kJ8Am7OODr7R",
        "outputId": "eb87ba4b-7086-4cfa-c3af-c3570c7b9815"
      },
      "source": [
        "dataset[0]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-55a9d71005c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ]
    }
  ]
}